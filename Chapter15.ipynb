{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the multiobjective least squares problem \n",
    "$$ \\text{minimize} \\sum_{i=1}^n \\lambda_i(x - b_i)^2 $$\n",
    "with $x\\in \\mathbf{R}$ we can simply take the derivative wrt $x$ and set it to 0 since the objective is convex.\n",
    "\n",
    "\\begin{align*}\n",
    "2\\sum_{i=1}^n \\lambda_i (x-b_i) &= 0\\\\\n",
    "x \\sum_{i=1}^n \\lambda_i &= \\sum_{i=1}^n\\lambda_i b_i\\\\\n",
    "x^{\\star} &= \\sum_{i=1}^n \\frac{\\lambda_i}{\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n} b_i\\\\\n",
    "\\end{align*}\n",
    "So the optimal $x$ is just a convex combination of the $b_i$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.2 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the minimizer of the regularized data fitting problem \n",
    "\n",
    "$$ ||A\\theta - y||^2 + \\lambda(\\theta_2^2 + \\theta_3^2 + \\cdots + \\theta_n^2)$$\n",
    " simply taking the gradient and setting to zero.\n",
    " \n",
    "\\begin{align*}\n",
    " 2A^TA\\theta - 2A^Ty + 2\\lambda \\theta - 2\\theta_1 \\mathbf{e}_1 = 0\\\\\n",
    " (A^TA+ \\lambda I - \\lambda \\mathbf{e}_1 \\mathbf{e}^T_1) \\cdot \\theta =  A^Ty\n",
    "\\end{align*}\n",
    "\n",
    "Including the bias term in the regularization penalty yields a solution that satisfies\n",
    "\n",
    "$$ (A^TA+ \\lambda I) \\cdot \\theta =  A^Ty$$\n",
    "\n",
    "Start by noting that the correlation matrix $A^TA$ is given by \n",
    "\n",
    "$$\n",
    "A^TA =\n",
    "\\begin{bmatrix}\n",
    "N & 0 & 0 & 0 & \\cdots & 0\\\\\n",
    "0 & 1 & \\rho_{12} & \\rho_{13} & \\cdots & \\rho_{1N}\\\\\n",
    "0 & \\rho_{21} & 1 & \\rho_{23} & \\cdots & \\rho_{2N}\\\\\n",
    "0 & \\rho_{31} & \\rho_{32} & 1 & \\cdots & \\rho_{3N}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\rho_{N1} & \\rho_{N2} & \\rho_{N3} & \\cdots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The regularization matrix for the first problem is then\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "N & 0 & 0 & 0 & \\cdots & 0\\\\\n",
    "0 & 1+\\lambda & \\rho_{12} & \\rho_{13} & \\cdots & \\rho_{1N}\\\\\n",
    "0 & \\rho_{21} & 1+\\lambda & \\rho_{23} & \\cdots & \\rho_{2N}\\\\\n",
    "0 & \\rho_{31} & \\rho_{32} & 1+\\lambda & \\cdots & \\rho_{3N}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\rho_{N1} & \\rho_{N2} & \\rho_{N3} & \\cdots & 1+\\lambda\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\vdots \\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "h_1\\\\\n",
    "h_2\\\\\n",
    "h_3\\\\\n",
    "\\vdots \\\\\n",
    "h_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Including the bias term in the regularization results in the following set of equations\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "N + \\lambda & 0 & 0 & 0 & \\cdots & 0\\\\\n",
    "0 & 1+\\lambda & \\rho_{12} & \\rho_{13} & \\cdots & \\rho_{1N}\\\\\n",
    "0 & \\rho_{21} & 1+\\lambda & \\rho_{23} & \\cdots & \\rho_{2N}\\\\\n",
    "0 & \\rho_{31} & \\rho_{32} & 1+\\lambda & \\cdots & \\rho_{3N}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\rho_{N1} & \\rho_{N2} & \\rho_{N3} & \\cdots & 1+\\lambda\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\vdots \\\\\n",
    "\\theta_p\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "h_1\\\\\n",
    "h_2\\\\\n",
    "h_3\\\\\n",
    "\\vdots \\\\\n",
    "h_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The solution for $\\theta_2^{\\star}\\ldots\\theta_p^{\\star}$ do not depend on $\\theta_1^{\\star}$ so we conlude the nonbias parameters are the same for both problems if the columns of $A$ have been standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.3 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the bicriterion objective problem \n",
    "\n",
    "$$||A_1x - b_1||^2 + \\lambda ||A_2x - b_2||^2$$\n",
    "\n",
    "the solution is denoted $\\hat{x}(\\lambda)$. If $\\mu, \\gamma \\in \\mathbf{R}_{++}$ are two different numbers and $\\hat{x}(\\mu) = \\hat{x}(\\gamma)$, from the definition we have that \n",
    "\n",
    "\\begin{align*}\n",
    "(A_1^TA_1 + \\gamma A_2^TA_2)\\hat{x}(\\gamma) = A_1^Tb_1 + \\gamma A_2^Tb_2\\\\\n",
    "(A_1^TA_1 + \\mu A_2^TA_2)\\hat{x}(\\gamma) = A_1^Tb_1 + \\mu A_2^Tb_2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Taking the difference of the two equations we have \n",
    "\n",
    "\\begin{align*}\n",
    "(\\gamma-\\mu) A_2^TA_2\\hat{x}(\\gamma) &= (\\gamma-\\mu) A_2^Tb_2\\\\\n",
    "\\implies A_2^TA_2\\hat{x}(\\gamma) &= A_2^Tb_2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Plugging back in, we find\n",
    "$$A_1^TA_1\\hat{x}(\\gamma) = A_1^Tb_1$$\n",
    "\n",
    "Now consider a different value $\\lambda > 0$ and the corresponding normal equations \n",
    "\n",
    "$$(A_1^TA_1 + \\lambda A_2^TA_2)\\hat{x}(\\lambda) = A_1^Tb_1 + \\lambda A_2^Tb_2\n",
    "$$\n",
    "\n",
    "Plugging in $\\hat{x}(\\gamma)$ we have \n",
    "\n",
    "\\begin{align*}\n",
    "(A_1^TA_1 + \\lambda A_2^TA_2)\\hat{x}(\\gamma) &= A_1^TA_1\\hat{x}(\\gamma) + \\lambda A_2^TA_2\\hat{x}(\\gamma)\\\\\n",
    "&= A_1^Tb_1 + \\lambda A_2^Tb_2\n",
    "\\end{align*}\n",
    "\n",
    "This demonstrates that $\\hat{x}(\\gamma) = \\hat{x}(\\lambda)$ for any $\\lambda \\in \\mathbf{R}_{++}$. This shows the solution is independent of the regularization parameter and the tradeoff curve is just a single point $(J^\\star_1(\\gamma), J^{\\star}_2(\\gamma))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing $\\hat{x}(\\lambda)$ is nonconstant and $\\lambda < \\mu$. Since $\\hat{x}(\\lambda)$ is the unique minimizer of $J_1(x) + \\lambda J_2$ the following inequalities immediately follow\n",
    "\n",
    "\\begin{align*}\n",
    "J_1^\\star (\\mu)+ \\lambda J_2^\\star(\\mu) &> J_1^\\star(\\lambda) + \\lambda J_2^\\star(\\lambda)\\\\\n",
    "J_1^\\star(\\lambda) + \\mu J_2^\\star(\\lambda) &> J_1^\\star(\\mu) + \\mu J_2^\\star(\\mu)\n",
    "\\end{align*}\n",
    "\n",
    "Adding the two inequalities we have \n",
    "\\begin{align*}\n",
    "J_1^\\star (\\mu)+ J_1^\\star(\\lambda)+ \\lambda J_2^\\star(\\mu)+\\mu J_2^\\star(\\lambda)  &> J_1^\\star(\\lambda) + \\lambda J_2^\\star(\\lambda)+ J_1^\\star(\\mu) + \\mu J_2^\\star(\\mu)\\\\\n",
    "\\implies (\\mu - \\lambda) J_2^\\star(\\lambda) &> (\\mu - \\lambda) J_2^\\star(\\mu)\\\\\n",
    "\\implies J_2^\\star(\\lambda) &> J_2^\\star(\\mu)\n",
    "\\end{align*}\n",
    "\n",
    "Plugging back into the first inequality we have \n",
    "\\begin{align*}\n",
    "J_1^\\star (\\mu)+ \\lambda J_2^\\star(\\lambda) &> J_1^\\star(\\lambda) + \\lambda J_2^\\star(\\lambda)\\\\\n",
    "\\implies J_1^\\star (\\mu) &> J_1^\\star(\\lambda)\n",
    "\\end{align*}\n",
    "\n",
    "These two results show that increasing the regularization weight will decrease $J_2$ and increase $J_1$\n",
    "\n",
    "Define \n",
    "$$ S = \\lim_{\\mu \\rightarrow \\lambda} \\frac{J_2^\\star(\\mu) - J_2^\\star(\\lambda)}{J_1^\\star(\\mu) - J_1^\\star(\\lambda)}$$\n",
    "\n",
    "Using the inequality above we know that \n",
    "\n",
    "\\begin{align*}\n",
    "S &\\le \\frac{-1/\\lambda J_1^\\star(\\mu) - J_2^\\star(\\lambda) + 1/\\lambda J_1^\\star(\\lambda) + J_2^\\star(\\lambda)}{J_1^\\star(\\mu) - J_1^\\star(\\lambda))}\\\\\n",
    "\\implies S &\\le \\frac{-1/\\lambda (J_1^\\star(\\mu) - J_1^\\star(\\lambda))}{J_1^\\star(\\mu) - J_1^\\star(\\lambda)}\\\\\n",
    "\\implies S &\\le -1/\\lambda\n",
    "\\end{align*}\n",
    "\n",
    "Using the other inequality we have \n",
    "\\begin{align*}\n",
    "S &\\ge \\frac{J_2^\\star(\\mu) - (1/\\lambda J_1^\\star(\\mu) + J_2^\\star(\\mu) -1/\\lambda J_1^\\star(\\lambda))}{J_1^\\star(\\mu) - J_1^\\star(\\lambda)}\\\\\n",
    "\\implies S &\\ge \\frac{-1/\\lambda (J_1^\\star(\\mu) - J_1^\\star(\\lambda))}{J_1^\\star(\\mu) - J_1^\\star(\\lambda)}\\\\\n",
    "\\implies S &\\ge -1/\\lambda\n",
    "\\end{align*}\n",
    "\n",
    "The combination of these inequalities demonstrates that $S=-1/\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 15.4 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $A\\in \\mathbf{R}^{m\\times n}$ and $D\\in \\mathbf{R}^{(n-1)\\times n}$ be the difference matrix with $i^{th}$ row $(e_{i+1}-e_i)^T$ and consider the weighted sum of least squares objective $||Ax-b||^2 + \\lambda||Dx||^2$. The minimizer of this objective is found by creating the stacked matrix\n",
    "$$\\tilde{A} = \n",
    "\\begin{bmatrix}\n",
    "A \\\\\n",
    "\\sqrt{\\lambda}D\n",
    "\\end{bmatrix}$$\n",
    "To prove this matrix is has linearly independent columns iff $A\\mathbf{1} \\ne 0$ we prove both cases. First, assume the columns of $\\tilde{A}$ are linearly independent then by definition we have \n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{A}\\mathbf{1} &\\ne 0\\\\\n",
    "\\begin{bmatrix}\n",
    "A\\mathbf{1} \\\\\n",
    "\\sqrt{\\lambda}D\\mathbf{1}\n",
    "\\end{bmatrix} &\\ne 0\\\\\n",
    "\\begin{bmatrix}\n",
    "A\\mathbf{1} \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "&\\ne 0\n",
    "\\end{align*}\n",
    "\n",
    "The last line implies $A\\mathbf{1} \\ne 0$. Next, assume $A\\mathbf{1} \\ne 0$. In order for $\\tilde{A}x = 0$ we must have $Dx =0$ which from simple backward substitution implies $x= \\alpha \\mathbf{1}$. Therefore all elements of the null space of $\\tilde{A}$ are of the form $x = \\alpha \\mathbf{1}$. Since $\\tilde{A}x =0$ requires $Ax=0$ we must have that $A\\mathbf{1} =0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 15.5 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a demand model of the form \n",
    "$$\\delta^d = E\\delta^p$$\n",
    "where $\\delta^d \\in \\mathbf{R}^n$ is the percentage change in demand from its nominal value and likewise for $\\delta^p\\in \\mathbf{R}^n$ we can estimate the elasticity matrix given data $(p_1,d_1),(p_2,d_2)\\ldots (p_n,d_n)$ using matrix least squares.\n",
    "\n",
    "We first form the optimization problem\n",
    "$$\\text{minimize}_{E} \\sum_{i=1}^N ||E\\delta_i^p - \\delta_i^d||^2$$\n",
    "where $\\delta_i$ is the vector of percentage differences from the nominal values.\n",
    "\n",
    "This problem can reformulated as\n",
    "$$\\text{minimize}_E\\ || EP - D||_F^2$$\n",
    "leading to the equivalent problem \n",
    "$$\\text{minimize}_E\\ ||P^TE^T -D^T|_F^2$$\n",
    "which can be solved using the normal equations as \n",
    "$$E^{\\star} = DP^T(PP^T)^{-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.6 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the linear dynamical system of the form $x_{t+1} = Ax_t + Bu_t$ to solve the control problem\n",
    "\n",
    "$$\\text{minimize}_{u_t}\\quad ||x_{t+1}||^2 + \\rho ||u_t||^2$$\n",
    "\n",
    "we rewrite the objective as\n",
    "$$\\text{minimize}_{u_t} \\quad ||Bu_t + Ax_t||^2 + \\rho ||u_t||^2$$\n",
    "\n",
    "which is a simple regularized least squares problem with solution\n",
    "\n",
    "$$u^\\star_t = -(B^TB + \\rho I)^{-1}B^TAx_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.7 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the weighted gram matrix $G = \\lambda_1 A_1^TA_1 + \\lambda_2 A_2^TA_2 + \\cdots + \\lambda_k A_k^TA_k$ where $\\lambda_i = 0 \\, \\forall i\\in \\{1,\\ldots,k\\}$. $G$ is invertible if there is no nonzero $x$ satisfying $A_1x,\\ldots,A_kx=0$ since we can show that $G$ will be positive definite in this case.\n",
    "\n",
    "\\begin{align*}\n",
    "    x^TGx &= x^T(\\lambda_1 A_1^TA_1 + \\lambda_2 A_2^TA_2 + \\cdots + \\lambda_k A_k^TA_k)x\\\\\n",
    "          &= \\lambda_1 x^TA_1^TA_1x + \\lambda_2 x^TA_2^TA_2x + \\cdots + \\lambda_k x^TA_k^TA_kx\\\\\n",
    "          &= \\lambda_1 ||A_1x||^2 + \\lambda_2 ||A_2x||^2 + \\cdots + \\lambda_k ||A_kx||^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since $A_ix$ is never zero except when $x=0$ this, quantity is always positive demonstrating $G\\succ 0$ and therefore invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 15.8 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the stratified model with regularization penalty\n",
    "\n",
    "$$ \\left\\lVert A^{(1)}x - y^{(1)}\\right\\lVert^2 + \\left\\lVert A^{(2)}x - y^{(2)}\\right\\lVert^2 + \\lambda \\left\\lVert\\theta^{(1)} - \\theta^{(2)}\\right\\lVert^2$$\n",
    "\n",
    "which essentially splits the model into two based on some feature (e.g. men and women) where the regularization penalizes models for the two different groups being different.\n",
    "\n",
    "The optimization problem can be written as a standard least squares problem as \n",
    "\n",
    "$$\\text{minimize}\\quad\n",
    "\\left\\lVert\n",
    "\\begin{bmatrix}\n",
    "A^{(1)} & 0\\\\\n",
    "0 & A^{(2)}\\\\\n",
    "\\sqrt{\\lambda}I & -\\sqrt{\\lambda}I\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta^{(1)}\\\\\n",
    "\\theta^{(2)}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\right\\lVert^2\n",
    "$$\n",
    "\n",
    "If the strata instead represented age group buckets and we desired similar models for similar age groups we could instead minimize the following multiobjective least squares problem\n",
    "$$\\sum_{i=1}^N \\left\\lVert A^{(i)}x - y^{(i)}\\right\\lVert ^2 + \\lambda\\sum_{i=1}^{N-1} \\left\\lVert \\theta^{(i)} - \\theta^{(i+1)} \\right\\lVert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.10 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the time series $y \\in \\mathbf{R}^{K\\cdot P}$ that we suspect is a noisy observation of a $P$ periodic underlying signal we can find this series by solving the least squares problem\n",
    "$$\\text{minimize }_x \\left\\lVert y - [I\\ I\\ \\ldots\\ I]^T x\\right\\lVert^2$$\n",
    "\n",
    "where $I\\in \\mathbf{R}^{P \\times P}$. The problem can be solved by simply substituting into the normal equations\n",
    "\n",
    "\\begin{align*}\n",
    "x^\\star &= (A^TA)^{-1}A^Ty\\\\\n",
    "&= \\left([I\\ I\\ \\ldots\\ I] [I\\ I\\ \\ldots\\ I]^T\\right)^{-1}[I\\ I\\ \\ldots\\ I]y \\\\\n",
    "&= (KI)^{-1}[I\\ I\\ \\ldots\\ I]y \\\\\n",
    "&= \\frac{1}{K} \\left(y_{1:P} + y_{P+1:2P} + \\cdots y_{(K-1)P+1:KP}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In summary, the least squares estimate of each component of $x$ is \n",
    "$$x^\\star_i = \\frac{1}{K} \\sum_{k=0}^{K-1} y_{i + K\\cdot P}$$\n",
    "which is just the average of the corresponding indices of the original vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 15.11 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moore-Penrose pseudo-inverse is defined as \n",
    "$$A^{\\dagger} = \\lim_{\\lambda \\rightarrow 0}\\ (A^TA +\\lambda I)^{-1}A^T = \\lim_{\\lambda \\rightarrow 0}\\ A^T(AA^T +\\lambda I)^{-1}$$\n",
    "\n",
    "The pseudo-inverse of $0\\in \\mathbf{R}^{m\\times n}$ is then \n",
    "\\begin{align*}\n",
    "A^{\\dagger} &= \\lim_{\\lambda \\rightarrow 0}\\ (\\lambda I)^{-1}0^T \\\\\n",
    "&= \\lim_{\\lambda \\rightarrow 0}\\ 0^T\\\\\n",
    "&= 0_{n\\times m}\n",
    "\\end{align*}\n",
    "\n",
    "When the columns are linearly independent then from the hint (that the limit of the inverse is the inverse of the limit, provided it exists) we have\n",
    "\n",
    "\\begin{align*}\n",
    "A^{\\dagger} &= \\lim_{\\lambda \\rightarrow 0}\\ (A^TA + \\lambda I)^{-1}A^T \\\\\n",
    "&= (\\lim_{\\lambda \\rightarrow 0}\\ A^TA + \\lambda I)^{-1}A^T\\\\\n",
    "&= (A^TA)^{-1}A^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For the case of linearly independent rows, we have\n",
    "\n",
    "\\begin{align*}\n",
    "A^{\\dagger} &= \\lim_{\\lambda \\rightarrow 0}\\ A^T(AA^T +\\lambda I)^{-1} \\\\\n",
    "&= A^T(\\lim_{\\lambda \\rightarrow 0}\\ AA^T +\\lambda I)^{-1}\\\\\n",
    "&= A^T(AA^T)^{-1}\\\\\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
